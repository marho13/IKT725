{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"IKT_725_HW3.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"xdVB18buMj0R"},"source":["# Code modified from the book \"Grokking deep reinforcement learning\" of Miguel Morales\n","# DDPG, TD3"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"H63DJ6DGMj0U","executionInfo":{"status":"ok","timestamp":1630312742450,"user_tz":-120,"elapsed":318,"user":{"displayName":"Juan Cardenas","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgQjLHUzuYa7oFgsJ4-id7XpKvfn1wzYDblUNtBHFI=s64","userId":"06662146926909775044"}},"outputId":"f90d73f3-8a8f-4335-8153-f5565e615358"},"source":["# Check if GPU is connected\n","# ---DO NOT TOUCH---\n","!nvidia-smi"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mon Aug 30 08:39:02 2021       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 470.57.02    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   37C    P8     9W /  70W |      0MiB / 15109MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"JdxmVVD2Mj0X"},"source":["# Libraries and setup\n","# ---DO NOT TOUCH---\n","import warnings ; warnings.filterwarnings('ignore')\n","import os\n","os.environ['CUDA_DEVICE_ORDER']='PCI_BUS_ID'\n","os.environ['OMP_NUM_THREADS'] = '1'\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import torch.multiprocessing as mp\n","import threading\n","from torch.distributions import Normal\n","\n","import numpy as np\n","from IPython.display import display\n","from collections import namedtuple, deque\n","import matplotlib.pyplot as plt\n","import matplotlib.pylab as pylab\n","from itertools import cycle, count\n","from textwrap import wrap\n","\n","import matplotlib\n","import subprocess\n","import os.path\n","import tempfile\n","import random\n","import base64\n","import pprint\n","import glob\n","import time\n","import json\n","import sys\n","import gym\n","import io\n","import os\n","import gc\n","\n","from gym import wrappers\n","from skimage.transform import resize\n","from skimage.color import rgb2gray\n","from subprocess import check_output\n","from IPython.display import display, HTML\n","\n","LEAVE_PRINT_EVERY_N_SECS = 300\n","ERASE_LINE = '\\x1b[2K'\n","EPS = 1e-6\n","BEEP = lambda: os.system(\"printf '\\a'\")\n","RESULTS_DIR = os.path.join('..', 'results')\n","SEEDS = (12, 34, 56, 78, 90)\n","\n","%matplotlib inline"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"D4f5LfetMj0Y"},"source":["# Setup plotting parameters\n","# ---DO NOT TOUCH---\n","plt.style.use('fivethirtyeight')\n","params = {\n","    'figure.figsize': (15, 8),\n","    'font.size': 24,\n","    'legend.fontsize': 20,\n","    'axes.titlesize': 28,\n","    'axes.labelsize': 24,\n","    'xtick.labelsize': 20,\n","    'ytick.labelsize': 20\n","}\n","pylab.rcParams.update(params)\n","np.set_printoptions(suppress=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qlTz_Z2IMj0a","executionInfo":{"status":"ok","timestamp":1630312747779,"user_tz":-120,"elapsed":8,"user":{"displayName":"Juan Cardenas","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgQjLHUzuYa7oFgsJ4-id7XpKvfn1wzYDblUNtBHFI=s64","userId":"06662146926909775044"}},"outputId":"0474ef24-9d62-46d4-d9b0-af71670c7390"},"source":["# Check for GPU acceleration \n","# Remember to enable it in the menu Runtime/Change runtime type\n","# ---DO NOT TOUCH---\n","torch.cuda.is_available()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":5}]},{"cell_type":"code","metadata":{"id":"4SUiircRMj0a"},"source":["# Get gym environment function\n","# ---DO NOT TOUCH---\n","def get_make_env_fn(**kargs):\n","    def make_env_fn(env_name, seed=None, render=None, record=False,\n","                    unwrapped=False, monitor_mode=None, \n","                    inner_wrappers=None, outer_wrappers=None):\n","        mdir = tempfile.mkdtemp()\n","        env = None\n","        if render:\n","            try:\n","                env = gym.make(env_name, render=render)\n","            except:\n","                pass\n","        if env is None:\n","            env = gym.make(env_name)\n","        if seed is not None: env.seed(seed)\n","        env = env.unwrapped if unwrapped else env\n","        if inner_wrappers:\n","            for wrapper in inner_wrappers:\n","                env = wrapper(env)\n","        env = wrappers.Monitor(\n","            env, mdir, force=True, \n","            mode=monitor_mode, \n","            video_callable=lambda e_idx: record) if monitor_mode else env\n","        if outer_wrappers:\n","            for wrapper in outer_wrappers:\n","                env = wrapper(env)\n","        return env\n","    return make_env_fn, kargs"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"laQjcJW7Mj0d"},"source":["# DDPG"]},{"cell_type":"code","metadata":{"id":"a6N3CAcMMj0f"},"source":["# Q-value neural network class\n","# There is a task here!\n","\n","class FCQV(nn.Module):\n","    def __init__(self, \n","                 input_dim, \n","                 output_dim):\n","        \"\"\"\n","        Class initialization\n","\n","        input_dim = input dimension\n","        output_dim = output dimension\n","\n","        hidden_dims = dimension for hidden layers\n","        activation_fc = activation function\n","\n","        device = processing device\n","        \"\"\"\n","        super(FCQV, self).__init__()\n","\n","        #TODO: Choose a non-linear activation function from https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity\n","        activation_fc = #To complete. Use the format ---> F.activation_function\n","        self.activation_fc = activation_fc\n","\n","        #TODO: propose the dimensions for the hidden layers\n","        hidden_dims = #To complete. Use the format (dimension_1, ..., dimension_n)\n","\n","        self.input_layer = nn.Linear(input_dim, hidden_dims[0])\n","        self.hidden_layers = nn.ModuleList()\n","        for i in range(len(hidden_dims)-1):\n","            in_dim = hidden_dims[i]\n","            if i == 0: \n","                in_dim += output_dim\n","            hidden_layer = nn.Linear(in_dim, hidden_dims[i+1])\n","            self.hidden_layers.append(hidden_layer)\n","        self.output_layer = nn.Linear(hidden_dims[-1], 1)\n","\n","        device = \"cpu\"\n","        if torch.cuda.is_available():\n","            device = \"cuda:0\"\n","        self.device = torch.device(device)\n","        self.to(self.device)\n","    \n","    def _format(self, state, action):\n","        \"\"\"\n","        Format the state for pytorch\n","\n","        state = state from environment\n","        action = action from policy\n","        \"\"\"\n","        x, u = state, action\n","        if not isinstance(x, torch.Tensor):\n","            x = torch.tensor(x, \n","                             device=self.device, \n","                             dtype=torch.float32)\n","            x = x.unsqueeze(0)\n","        if not isinstance(u, torch.Tensor):\n","            u = torch.tensor(u, \n","                             device=self.device, \n","                             dtype=torch.float32)\n","            u = u.unsqueeze(0)\n","        return x, u\n","\n","    def forward(self, state, action):\n","        \"\"\"\n","        Forward function for neural network\n","\n","        state = state from environment\n","        action = action from policy\n","        \"\"\"\n","        x, u = self._format(state, action)\n","        x = self.activation_fc(self.input_layer(x))\n","        for i, hidden_layer in enumerate(self.hidden_layers):\n","            if i == 0:\n","                x = torch.cat((x, u), dim=1)\n","            x = self.activation_fc(hidden_layer(x))\n","        return self.output_layer(x)\n","    \n","    def load(self, experiences):\n","        \"\"\"\n","        load samples from experience - replay buffer database\n","\n","        experiences = samples from the replay buffer database\n","        \"\"\"\n","        states, actions, new_states, rewards, is_terminals = experiences\n","        states = torch.from_numpy(states).float().to(self.device)\n","        actions = torch.from_numpy(actions).float().to(self.device)\n","        new_states = torch.from_numpy(new_states).float().to(self.device)\n","        rewards = torch.from_numpy(rewards).float().to(self.device)\n","        is_terminals = torch.from_numpy(is_terminals).float().to(self.device)\n","        return states, actions, new_states, rewards, is_terminals"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Srz0p3ugMj0f"},"source":["# Deterministic policy neural network class\n","# There is a task here!\n","\n","class FCDP(nn.Module):\n","    def __init__(self, \n","                 input_dim,\n","                 action_bounds,\n","                 out_activation_fc=F.tanh):\n","      \"\"\"\n","      Class initialization\n","\n","      input_dim = input dimension\n","      output_dim = output dimension\n","\n","      hidden_dims = dimension for hidden layers\n","      activation_fc = activation function\n","      out_activation_fc = Output activation function\n","\n","      device = processing device\n","      \"\"\"\n","      super(FCDP, self).__init__()\n","\n","      #TODO: Choose a non-linear activation function from https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity\n","      activation_fc = #To complete. Use the format ---> F.activation_function\n","\n","      self.activation_fc = activation_fc\n","      self.out_activation_fc = out_activation_fc\n","      self.env_min, self.env_max = action_bounds\n","      \n","      #TODO: propose the dimensions for the hidden layers\n","      hidden_dims = #To complete. Use the same format (dimension_1, ..., dimension_n)\n","\n","      self.input_layer = nn.Linear(input_dim, hidden_dims[0])\n","      self.hidden_layers = nn.ModuleList()\n","      for i in range(len(hidden_dims)-1):\n","          hidden_layer = nn.Linear(hidden_dims[i], hidden_dims[i+1])\n","          self.hidden_layers.append(hidden_layer)\n","      self.output_layer = nn.Linear(hidden_dims[-1], len(self.env_max))\n","\n","      device = \"cpu\"\n","      if torch.cuda.is_available():\n","          device = \"cuda:0\"\n","      self.device = torch.device(device)\n","      self.to(self.device)\n","      \n","      self.env_min = torch.tensor(self.env_min,\n","                                  device=self.device, \n","                                  dtype=torch.float32)\n","\n","      self.env_max = torch.tensor(self.env_max,\n","                                  device=self.device, \n","                                  dtype=torch.float32)\n","      \n","      self.nn_min = self.out_activation_fc(\n","          torch.Tensor([float('-inf')])).to(self.device)\n","      self.nn_max = self.out_activation_fc(\n","          torch.Tensor([float('inf')])).to(self.device)\n","      self.rescale_fn = lambda x: (x - self.nn_min) * (self.env_max - self.env_min) / \\\n","                                  (self.nn_max - self.nn_min) + self.env_min\n","\n","    def _format(self, state):\n","      \"\"\"\n","      Format the state for pytorch\n","\n","      state = current state\n","      \"\"\"\n","      x = state\n","      if not isinstance(x, torch.Tensor):\n","          x = torch.tensor(x, \n","                            device=self.device, \n","                            dtype=torch.float32)\n","          x = x.unsqueeze(0)\n","      return x\n","\n","    def forward(self, state):\n","      \"\"\"\n","      Forward function for neural network\n","\n","      state = current state\n","      \"\"\"\n","      x = self._format(state)\n","      x = self.activation_fc(self.input_layer(x))\n","      for hidden_layer in self.hidden_layers:\n","          x = self.activation_fc(hidden_layer(x))\n","      x = self.output_layer(x)\n","      x = self.out_activation_fc(x)\n","      return self.rescale_fn(x)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WPVfkGXGMj0g"},"source":["# Replay buffer class\n","# There is a task here!\n","\n","class ReplayBuffer():\n","    def __init__(self, \n","                 max_size=10000, \n","                 batch_size=64):\n","      \"\"\"\n","      Initialize class \n","\n","      ss_mem = state buffer\n","      as_mem = action buffer\n","      rs_mem = reward buffer\n","      ps_mem = probability buffer\n","      ds_mem = discount factor buffer\n","\n","      max_size = maximum buffer size\n","      batch_size = sample batch size\n","      _idx = buffer index\n","      size = ongoing buffer size\n","      \"\"\"\n","      self.ss_mem = np.empty(shape=(max_size), dtype=np.ndarray)\n","      self.as_mem = np.empty(shape=(max_size), dtype=np.ndarray)\n","      self.rs_mem = np.empty(shape=(max_size), dtype=np.ndarray)\n","      self.ps_mem = np.empty(shape=(max_size), dtype=np.ndarray)\n","      self.ds_mem = np.empty(shape=(max_size), dtype=np.ndarray)\n","\n","      self.max_size = max_size\n","      self.batch_size = batch_size\n","      self._idx = 0\n","      self.size = 0\n","    \n","    def store(self, sample):\n","      \"\"\"\n","      get values from the samples\n","\n","      sample = tuple (s, a, r, p, d)\n","      s = state\n","      a = action\n","      r = reward\n","      p = new_state\n","      d = flag for terminal state\n","\n","      ss_mem = memory buffer for current states\n","      as_mem = memory buffer for actions\n","      rs_mem = memory buffer for rewards\n","      ps_mem = memory buffer for new states\n","      ds_mem = memory buffer for terminal state flag\n","      \"\"\"\n","      s, a, r, p, d = sample\n","      \n","      #TODO: Complete the function store in the relay buffer class\n","      self.ss_mem[self._idx] = # To complete\n","      self.as_mem[self._idx] = # To complete\n","      self.rs_mem[self._idx] = # To complete\n","      self.ps_mem[self._idx] = # To complete\n","      self.ds_mem[self._idx] = # To complete\n","      \n","      self._idx += 1\n","      self._idx = self._idx % self.max_size\n","\n","      self.size += 1\n","      self.size = min(self.size, self.max_size)\n","\n","    def sample(self, batch_size=None):\n","      \"\"\"\n","      store samples in the buffer\n","\n","      idxs = index\n","      experiences = samples in buffer\n","      \"\"\"\n","      if batch_size == None:\n","          batch_size = self.batch_size\n","\n","      idxs = np.random.choice(\n","          self.size, batch_size, replace=False)\n","      experiences = np.vstack(self.ss_mem[idxs]), \\\n","                    np.vstack(self.as_mem[idxs]), \\\n","                    np.vstack(self.rs_mem[idxs]), \\\n","                    np.vstack(self.ps_mem[idxs]), \\\n","                    np.vstack(self.ds_mem[idxs])\n","      return experiences\n","\n","    def __len__(self):\n","      \"\"\"\n","      get buffer size\n","      \"\"\"\n","      return self.size"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UQG95rrMMj0g"},"source":["# Compute the greedy strategy\n","# There is a task here!\n","\n","class GreedyStrategy():\n","\n","    def __init__(self, bounds):\n","      \"\"\"\n","      Initialize class \n","\n","      bounds = upper and lower bounds for action\n","      low = lower bound for action\n","      high = upper bound for action\n","      ratio_noise_injected = noise in deterministic greedy policy for exploration\n","      \"\"\"\n","      self.low, self.high = bounds\n","      self.ratio_noise_injected = 0\n","\n","    def select_action(self, model, state):\n","      \"\"\"\n","      Select greedy action\n","\n","      model = policy model\n","      state = current state\n","\n","      greedy_action = compute action from the policy\n","      action = action after clippping\n","      \"\"\"\n","      with torch.no_grad():\n","          greedy_action = model(state).cpu().detach().data.numpy().squeeze()\n","\n","      #TODO use the np.clip function to clip the greedy action between the values 'low' and 'high'\n","      action = # To complete. You may find more information about the clip function in https://numpy.org/doc/stable/reference/generated/numpy.clip.html\n","      return np.reshape(action, self.high.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dMTYIuuzMj0h"},"source":["# Compute noise for exploration\n","# There is a task here!\n","\n","class NormalNoiseStrategy():\n","    def __init__(self, bounds, exploration_noise_ratio=0.1):\n","      \"\"\"\n","      class initialization\n","\n","      bounds = upper and lower bounds for noise\n","      exploration_noise_ratio = noise exploration ration\n","      \"\"\"\n","      self.low, self.high = bounds\n","      self.exploration_noise_ratio = exploration_noise_ratio\n","      self.ratio_noise_injected = 0\n","\n","    def select_action(self, model, state, max_exploration=False):\n","      \"\"\"\n","      select DDPG policy\n","\n","      model = policy model\n","      state = current state\n","      max_exploration = exploration strategy options\n","\n","      noise_scale = standard deviation for normal distribution\n","      noise = noise for exploration\n","      noisy_action = DDPG action with exploration noise\n","      action = clipped DDPG action\n","      \"\"\"\n","      if max_exploration:\n","          noise_scale = self.high\n","      else:\n","          noise_scale = self.exploration_noise_ratio * self.high\n","\n","      with torch.no_grad():\n","          greedy_action = model(state).cpu().detach().data.numpy().squeeze()\n","\n","      noise = np.random.normal(loc=0, scale=noise_scale, size=len(self.high))\n","\n","      #TODO: compute the DDPG action with an exploration strategy. Use the values greedy_action and noise\n","      noisy_action = #To complete\n","\n","      #TODO use the np.clip function to clip the DDPG action between the values 'self.low' and 'self.high'\n","      action = # To complete. You may find more information about the clip function in https://numpy.org/doc/stable/reference/generated/numpy.clip.html\n","      \n","      self.ratio_noise_injected = np.mean(abs((greedy_action - action)/(self.high - self.low)))\n","      return action"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1QAxU7TWMj0h"},"source":["# DDPG agent class\n","# There is a task here!\n","\n","class DDPG():\n","    def __init__(self, \n","                 replay_buffer_fn,\n","                 policy_model_fn, \n","                 policy_max_grad_norm, \n","                 policy_optimizer_fn, \n","                 policy_optimizer_lr,\n","                 value_model_fn, \n","                 value_max_grad_norm, \n","                 value_optimizer_fn, \n","                 value_optimizer_lr, \n","                 training_strategy_fn,\n","                 evaluation_strategy_fn,\n","                 n_warmup_batches,\n","                 update_target_every_steps,\n","                 tau):\n","        \"\"\"\n","        Class initialization\n","\n","        replay_buffer_fn = replay buffer function\n","\n","        policy_model_fn = policy neural network architecture\n","        policy_max_grad_norm = maximum gradient norm for policy model\n","        policy_optimizer_fn = optimizer for policy neural network\n","        policy_optimizer_lr = learning rate for policy neural network\n","\n","        value_model_fn = value function neural network architecture\n","        value_max_grad_norm = maximum gradient norm for Q-value model\n","        value_optimizer_fn = optimizer for value function neural network\n","        value_optimizer_lr = learning rate for value function neural network\n","\n","        training_strategy_fn = exploration strategy - Normal Noise Strategy\n","        evaluation_strategy_fn = evaluation strategy - Greedy Strategy\n","        n_warmup_batches = warm up batches for training\n","        update_target_every_steps = updating rate\n","        tau = Polyak averaging factor\n","        \"\"\"\n","        self.replay_buffer_fn = replay_buffer_fn\n","\n","        self.policy_model_fn = policy_model_fn\n","        self.policy_max_grad_norm = policy_max_grad_norm\n","        self.policy_optimizer_fn = policy_optimizer_fn\n","        self.policy_optimizer_lr = policy_optimizer_lr\n","        \n","        self.value_model_fn = value_model_fn\n","        self.value_max_grad_norm = value_max_grad_norm\n","        self.value_optimizer_fn = value_optimizer_fn\n","        self.value_optimizer_lr = value_optimizer_lr\n","\n","        self.training_strategy_fn = training_strategy_fn\n","        self.evaluation_strategy_fn = evaluation_strategy_fn\n","\n","        self.n_warmup_batches = n_warmup_batches\n","        self.update_target_every_steps = update_target_every_steps\n","        self.tau = tau\n","\n","    def optimize_model(self, experiences):\n","      \"\"\"\n","      Optimize and update parameters in neural network models (Q value and policy models)\n","\n","      experiences= experience buffer replay - Database\n","      \n","      argmax_a_q_sp = greedy policy for next state\n","      max_a_q_sp = max Q value function for next state\n","      target_q_sa = target Q value function\n","      q_sa = current Q value function\n","      td_error = TD error\n","      value_loss = loss function for Q value function neural network model\n","\n","      argmax_a_q_s = greedy action for current state\n","      max_a_q_s = Q value from greedy action in current state\n","      policy_loss = loss function for policy neural network model\n","      \"\"\"\n","\n","      states, actions, rewards, next_states, is_terminals = experiences\n","      batch_size = len(is_terminals)\n","\n","      # Compute greedy policy for next state\n","      argmax_a_q_sp = self.target_policy_model(next_states)\n","\n","      # Compute max Q value function for next state\n","      max_a_q_sp = self.target_value_model(next_states, argmax_a_q_sp)\n","\n","      # Update target Q value function\n","      target_q_sa = rewards + self.gamma * max_a_q_sp * (1 - is_terminals)\n","\n","      # Compute current Q value function\n","      q_sa = self.online_value_model(states, actions)\n","\n","      #TODO: compute TD error with target_q_sa and q_sa\n","      td_error = # To complete ---> When using the target_q_sa variable, you may want to work with target_q_sa.detach()\n","\n","      #TODO: compute the value loss function to train the neural network (check out slide 35 from lecture 13)\n","      value_loss = # To complete ---> Recall that we are dealing with td error samples, don't forget to compute the expectation via Monte Carlo\n","\n","      # Backpropagation for value function neural network\n","      self.value_optimizer.zero_grad()\n","      value_loss.backward()\n","      torch.nn.utils.clip_grad_norm_(self.online_value_model.parameters(), \n","                                      self.value_max_grad_norm)\n","      self.value_optimizer.step()\n","\n","      # Compute greedy action with policy model for current state\n","      argmax_a_q_s = self.online_policy_model(states)\n","\n","      # Get Q value from greedy action and current state\n","      max_a_q_s = self.online_value_model(states, argmax_a_q_s)\n","\n","      # Compute loss for policy model\n","      policy_loss = -max_a_q_s.mean()\n","\n","      # Backpropagation for policy neural network\n","      self.policy_optimizer.zero_grad()\n","      policy_loss.backward()\n","      torch.nn.utils.clip_grad_norm_(self.online_policy_model.parameters(), \n","                                      self.policy_max_grad_norm)        \n","      self.policy_optimizer.step()\n","\n","    def interaction_step(self, state, env):\n","      \"\"\"\n","      agent interacts with the environment by applying action\n","\n","      min_samples = database from the buffer replay - Train neural networks\n","      action = action from the policy model\n","      new_state = new state in the environment\n","      reward = reward from the current interaction\n","      is_failure = flag for terminal state\n","      experience = new sample tuple (state, action, reward, new_state, is_failure) for buffer replay\n","      \"\"\"\n","      min_samples = self.replay_buffer.batch_size * self.n_warmup_batches\n","      action = self.training_strategy.select_action(self.online_policy_model, \n","                                                    state, \n","                                                    len(self.replay_buffer) < min_samples)\n","      new_state, reward, is_terminal, info = env.step(action)\n","      is_truncated = 'TimeLimit.truncated' in info and info['TimeLimit.truncated']\n","      is_failure = is_terminal and not is_truncated\n","      experience = (state, action, reward, new_state, float(is_failure))\n","\n","      self.replay_buffer.store(experience)\n","      self.episode_reward[-1] += reward\n","      self.episode_timestep[-1] += 1\n","      self.episode_exploration[-1] += self.training_strategy.ratio_noise_injected\n","      return new_state, is_terminal\n","    \n","    def update_networks(self, tau=None):\n","      \"\"\"\n","      Update neural network models parameters (policy and Q-value function) via Polyak Averaging\n","      * We use this technique to avoid aggressive model parameters updates\n","\n","      tau = Polyak averaging factor\n","      target_ratio = averaging ratio\n","      mixed_weights = new mixed weights\n","      \"\"\"\n","      tau = self.tau if tau is None else tau\n","\n","      for target, online in zip(self.target_value_model.parameters(), \n","                                self.online_value_model.parameters()):\n","          target_ratio = (1.0 - self.tau) * target.data\n","          online_ratio = self.tau * online.data\n","          mixed_weights = target_ratio + online_ratio\n","          target.data.copy_(mixed_weights)\n","\n","      for target, online in zip(self.target_policy_model.parameters(), \n","                                self.online_policy_model.parameters()):\n","          target_ratio = (1.0 - self.tau) * target.data\n","          online_ratio = self.tau * online.data\n","          mixed_weights = target_ratio + online_ratio\n","          target.data.copy_(mixed_weights)\n","\n","    def train(self, make_env_fn, make_env_kargs, seed, gamma, \n","              max_minutes, max_episodes, goal_mean_100_reward):\n","      \"\"\"\n","      Training function and computing stats \n","\n","      make_env_fn = make environment function\n","      make_env_kargs = arguments for make environment function\n","      seed = seed for random numbers\n","      gamma = discount factor\n","      max_minutes = maximum training time\n","      max_episodes = maximum training episodes\n","      goal_mean_100_reward = target reward goal\n","      \"\"\"\n","\n","      # Setup environment\n","      training_start, last_debug_time = time.time(), float('-inf')\n","\n","      self.checkpoint_dir = tempfile.mkdtemp()\n","      self.make_env_fn = make_env_fn\n","      self.make_env_kargs = make_env_kargs\n","      self.seed = seed\n","      self.gamma = gamma\n","      \n","      env = self.make_env_fn(**self.make_env_kargs, seed=self.seed)\n","      torch.manual_seed(self.seed) ; np.random.seed(self.seed) ; random.seed(self.seed)\n","  \n","      # Initialize actor-critic agent\n","      nS, nA = env.observation_space.shape[0], env.action_space.shape[0]\n","      action_bounds = env.action_space.low, env.action_space.high\n","      self.episode_timestep = []\n","      self.episode_reward = []\n","      self.episode_seconds = []\n","      self.evaluation_scores = []        \n","      self.episode_exploration = []\n","      \n","      # Setup target and online Q-value functions\n","      self.target_value_model = self.value_model_fn(nS, nA)\n","      self.online_value_model = self.value_model_fn(nS, nA)\n","\n","      # Setup target and online policy model\n","      self.target_policy_model = self.policy_model_fn(nS, action_bounds)\n","      self.online_policy_model = self.policy_model_fn(nS, action_bounds)\n","\n","      # Setup optimize and update parameters functions\n","      self.update_networks(tau=1.0)\n","      self.value_optimizer = self.value_optimizer_fn(self.online_value_model, \n","                                                      self.value_optimizer_lr)        \n","      self.policy_optimizer = self.policy_optimizer_fn(self.online_policy_model, \n","                                                        self.policy_optimizer_lr)\n","\n","      # Setup replay buffer, and training/evaluation strategies\n","      self.replay_buffer = self.replay_buffer_fn()\n","      self.training_strategy = training_strategy_fn(action_bounds)\n","      self.evaluation_strategy = evaluation_strategy_fn(action_bounds)\n","                  \n","      result = np.empty((max_episodes, 5))\n","      result[:] = np.nan\n","      training_time = 0\n","\n","      # Episodic interaction agent-environment \n","      for episode in range(1, max_episodes + 1):\n","          episode_start = time.time()\n","          \n","          state, is_terminal = env.reset(), False\n","          self.episode_reward.append(0.0)\n","          self.episode_timestep.append(0.0)\n","          self.episode_exploration.append(0.0)\n","\n","          for step in count():\n","              state, is_terminal = self.interaction_step(state, env)\n","\n","              min_samples = self.replay_buffer.batch_size * self.n_warmup_batches\n","              if len(self.replay_buffer) > min_samples:\n","                  experiences = self.replay_buffer.sample()\n","                  experiences = self.online_value_model.load(experiences)\n","                  self.optimize_model(experiences)\n","\n","              if np.sum(self.episode_timestep) % self.update_target_every_steps == 0:\n","                  self.update_networks()\n","\n","              if is_terminal:\n","                  gc.collect()\n","                  break\n","          \n","          # save stats\n","          #---DO NOT TOUCH---\n","          episode_elapsed = time.time() - episode_start\n","          self.episode_seconds.append(episode_elapsed)\n","          training_time += episode_elapsed\n","          evaluation_score, _ = self.evaluate(self.online_policy_model, env)\n","          self.save_checkpoint(episode-1, self.online_policy_model)\n","\n","          total_step = int(np.sum(self.episode_timestep))\n","          self.evaluation_scores.append(evaluation_score)\n","          \n","          mean_10_reward = np.mean(self.episode_reward[-10:])\n","          std_10_reward = np.std(self.episode_reward[-10:])\n","          mean_100_reward = np.mean(self.episode_reward[-100:])\n","          std_100_reward = np.std(self.episode_reward[-100:])\n","          mean_100_eval_score = np.mean(self.evaluation_scores[-100:])\n","          std_100_eval_score = np.std(self.evaluation_scores[-100:])\n","          lst_100_exp_rat = np.array(\n","              self.episode_exploration[-100:])/np.array(self.episode_timestep[-100:])\n","          mean_100_exp_rat = np.mean(lst_100_exp_rat)\n","          std_100_exp_rat = np.std(lst_100_exp_rat)\n","          \n","          wallclock_elapsed = time.time() - training_start\n","          result[episode-1] = total_step, mean_100_reward, \\\n","              mean_100_eval_score, training_time, wallclock_elapsed\n","          \n","          reached_debug_time = time.time() - last_debug_time >= LEAVE_PRINT_EVERY_N_SECS\n","          reached_max_minutes = wallclock_elapsed >= max_minutes * 60\n","          reached_max_episodes = episode >= max_episodes\n","          reached_goal_mean_reward = mean_100_eval_score >= goal_mean_100_reward\n","          training_is_over = reached_max_minutes or \\\n","                              reached_max_episodes or \\\n","                              reached_goal_mean_reward\n","          elapsed_str = time.strftime(\"%H:%M:%S\", time.gmtime(time.time() - training_start))\n","          debug_message = 'el {}, ep {:04}, ts {:07}, '\n","          debug_message += 'ar 10 {:05.1f}\\u00B1{:05.1f}, '\n","          debug_message += '100 {:05.1f}\\u00B1{:05.1f}, '\n","          debug_message += 'ex 100 {:02.1f}\\u00B1{:02.1f}, '\n","          debug_message += 'ev {:05.1f}\\u00B1{:05.1f}'\n","          debug_message = debug_message.format(\n","              elapsed_str, episode-1, total_step, mean_10_reward, std_10_reward, \n","              mean_100_reward, std_100_reward, mean_100_exp_rat, std_100_exp_rat,\n","              mean_100_eval_score, std_100_eval_score)\n","          print(debug_message, end='\\r', flush=True)\n","          if reached_debug_time or training_is_over:\n","              print(ERASE_LINE + debug_message, flush=True)\n","              last_debug_time = time.time()\n","          if training_is_over:\n","              if reached_max_minutes: print(u'--> reached_max_minutes \\u2715')\n","              if reached_max_episodes: print(u'--> reached_max_episodes \\u2715')\n","              if reached_goal_mean_reward: print(u'--> reached_goal_mean_reward \\u2713')\n","              break\n","\n","      # End training and save results      \n","      final_eval_score, score_std = self.evaluate(self.online_policy_model, env, n_episodes=100)\n","      wallclock_time = time.time() - training_start\n","      print('Training complete.')\n","      print('Final evaluation score {:.2f}\\u00B1{:.2f} in {:.2f}s training time,'\n","            ' {:.2f}s wall-clock time.\\n'.format(\n","                final_eval_score, score_std, training_time, wallclock_time))\n","      env.close() ; del env\n","      self.get_cleaned_checkpoints()\n","      return result, final_eval_score, training_time, wallclock_time\n","    \n","    def evaluate(self, eval_policy_model, eval_env, n_episodes=1):\n","      \"\"\"\n","      evaluate trained policy\n","      \n","      eval_policy_model = policy model to evaluate\n","      eval_env = environment to evaluate\n","      n_episodes = number of episodes to evaluate\n","      a = action\n","      s = current state\n","      r = reward\n","      d = next state\n","      \"\"\"\n","      rs = []\n","      for _ in range(n_episodes):\n","          s, d = eval_env.reset(), False\n","          rs.append(0)\n","          for _ in count():\n","              a = self.evaluation_strategy.select_action(eval_policy_model, s)\n","              s, r, d, _ = eval_env.step(a)\n","              rs[-1] += r\n","              if d: break\n","      return np.mean(rs), np.std(rs)\n","\n","    def get_cleaned_checkpoints(self, n_checkpoints=4):\n","      \"\"\"\n","      clean database for saving\n","      \"\"\"\n","      try: \n","          return self.checkpoint_paths\n","      except AttributeError:\n","          self.checkpoint_paths = {}\n","\n","      paths = glob.glob(os.path.join(self.checkpoint_dir, '*.tar'))\n","      paths_dic = {int(path.split('.')[-2]):path for path in paths}\n","      last_ep = max(paths_dic.keys())\n","      checkpoint_idxs = np.linspace(1, last_ep+1, n_checkpoints, endpoint=True, dtype=np.int)-1\n","\n","      for idx, path in paths_dic.items():\n","          if idx in checkpoint_idxs:\n","              self.checkpoint_paths[idx] = path\n","          else:\n","              os.unlink(path)\n","\n","      return self.checkpoint_paths\n","\n","    def save_checkpoint(self, episode_idx, model):\n","      \"\"\"\n","      Save model\n","      \"\"\"\n","      torch.save(model.state_dict(), \n","                  os.path.join(self.checkpoint_dir, 'model.{}.tar'.format(episode_idx)))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-H509RMbMj0i","scrolled":false,"outputId":"a012d28d-df00-4e4a-e6bc-8004193296b7"},"source":["# DDPG training/evaluation routine\n","# There is a task here\n","\n","ddpg_results = []\n","best_agent, best_eval_score = None, float('-inf')\n","\n","# Prepare environment\n","for seed in SEEDS:\n","    environment_settings = {\n","        'env_name': 'Pendulum-v0',\n","        'gamma': 0.99,\n","        'max_minutes': 5,\n","        'max_episodes': 500,\n","        'goal_mean_100_reward': -150\n","    }\n","\n","    # Setup DDPG agent \n","    #---Policy neural network---\n","    policy_model_fn = lambda nS, bounds: FCDP(nS, bounds)\n","    policy_max_grad_norm = float('inf')\n","\n","    #TODO: select an optimization algorithm for the policy neural network. For further information: https://pytorch.org/docs/stable/optim.html - Algorithms\n","    policy_optimizer_fn = lambda net, lr: # To complete -> Follow the format optim.Optimization_Algorithm(net.parameters(), lr=lr)\n","\n","    #TODO: select a suitable learning rate for the optimization algorithm\n","    policy_optimizer_lr = # To complete\n","\n","    #---Value function neural network---\n","    value_model_fn = lambda nS, nA: FCQV(nS, nA)\n","    value_max_grad_norm = float('inf')\n","\n","    #TODO: select an optimization algorithm for the value neural network. For further information: https://pytorch.org/docs/stable/optim.html - Algorithms\n","    value_optimizer_fn = lambda net, lr: # To complete -> Follow the format optim.Optimization_Algorithm(net.parameters(), lr=lr)\n","\n","    #TODO: select a suitable learning rate for the optimization algorithm\n","    value_optimizer_lr = # To complete\n","\n","    # Training/evaluation strategy\n","    training_strategy_fn = lambda bounds: NormalNoiseStrategy(bounds, exploration_noise_ratio=0.1)\n","    evaluation_strategy_fn = lambda bounds: GreedyStrategy(bounds)\n","\n","    replay_buffer_fn = lambda: ReplayBuffer(max_size=100000, batch_size=256)\n","    n_warmup_batches = 5\n","    update_target_every_steps = 1\n","    tau = 0.005\n","    \n","    env_name, gamma, max_minutes, \\\n","    max_episodes, goal_mean_100_reward = environment_settings.values()\n","\n","    # Update agent\n","    agent = DDPG(replay_buffer_fn,\n","                 policy_model_fn, \n","                 policy_max_grad_norm, \n","                 policy_optimizer_fn, \n","                 policy_optimizer_lr,\n","                 value_model_fn, \n","                 value_max_grad_norm, \n","                 value_optimizer_fn, \n","                 value_optimizer_lr, \n","                 training_strategy_fn,\n","                 evaluation_strategy_fn,\n","                 n_warmup_batches,\n","                 update_target_every_steps,\n","                 tau)\n","\n","    # Train/evaluate agent\n","    make_env_fn, make_env_kargs = get_make_env_fn(env_name=env_name)\n","\n","    result, final_eval_score, training_time, wallclock_time = agent.train(\n","        make_env_fn, make_env_kargs, seed, gamma, max_minutes, max_episodes, goal_mean_100_reward)\n","    \n","    # Save results\n","    ddpg_results.append(result)\n","\n","    if final_eval_score > best_eval_score:\n","        best_eval_score = final_eval_score\n","        best_agent = agent\n","\n","ddpg_results = np.array(ddpg_results)\n","_ = BEEP()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\u001b[2Kel 00:00:12, ep 0000, ts 0000200, ar 10 -1296.7±000.0, 100 -1296.7±000.0, ex 100 0.3±0.0, ev -1391.7±000.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Xe-cz25cMj0k"},"source":["# Save results from DDPG agent \n","# ---DO NOT TOUCH---\n","ddpg_max_t, ddpg_max_r, ddpg_max_s, \\\n","ddpg_max_sec, ddpg_max_rt = np.max(ddpg_results, axis=0).T\n","ddpg_min_t, ddpg_min_r, ddpg_min_s, \\\n","ddpg_min_sec, ddpg_min_rt = np.min(ddpg_results, axis=0).T\n","ddpg_mean_t, ddpg_mean_r, ddpg_mean_s, \\\n","ddpg_mean_sec, ddpg_mean_rt = np.mean(ddpg_results, axis=0).T\n","ddpg_x = np.arange(len(ddpg_mean_s))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"u9-edJJPMj0k","scrolled":false},"source":["# Plot results\n","# ---DO NOT TOUCH---\n","fig, axs = plt.subplots(2, 1, figsize=(15,10), sharey=False, sharex=True)\n","\n","# DDPG\n","axs[0].plot(ddpg_max_r, 'r', linewidth=1)\n","axs[0].plot(ddpg_min_r, 'r', linewidth=1)\n","axs[0].plot(ddpg_mean_r, 'r:', label='DDPG', linewidth=2)\n","axs[0].fill_between(\n","    ddpg_x, ddpg_min_r, ddpg_max_r, facecolor='r', alpha=0.3)\n","\n","axs[1].plot(ddpg_max_s, 'r', linewidth=1)\n","axs[1].plot(ddpg_min_s, 'r', linewidth=1)\n","axs[1].plot(ddpg_mean_s, 'r:', label='DDPG', linewidth=2)\n","axs[1].fill_between(\n","    ddpg_x, ddpg_min_s, ddpg_max_s, facecolor='r', alpha=0.3)\n","\n","# ALL\n","axs[0].set_title('Moving Avg Reward (Training)')\n","axs[1].set_title('Moving Avg Reward (Evaluation)')\n","plt.xlabel('Episodes')\n","axs[0].legend(loc='upper left')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hbAzSYRJMj0l"},"source":["# Plot results\n","# ---DO NOT TOUCH---\n","fig, axs = plt.subplots(3, 1, figsize=(15,15), sharey=False, sharex=True)\n","\n","# DDPG\n","axs[0].plot(ddpg_max_t, 'r', linewidth=1)\n","axs[0].plot(ddpg_min_t, 'r', linewidth=1)\n","axs[0].plot(ddpg_mean_t, 'r:', label='DDPG', linewidth=2)\n","axs[0].fill_between(\n","    ddpg_x, ddpg_min_t, ddpg_max_t, facecolor='r', alpha=0.3)\n","\n","axs[1].plot(ddpg_max_sec, 'r', linewidth=1)\n","axs[1].plot(ddpg_min_sec, 'r', linewidth=1)\n","axs[1].plot(ddpg_mean_sec, 'r:', label='DDPG', linewidth=2)\n","axs[1].fill_between(\n","    ddpg_x, ddpg_min_sec, ddpg_max_sec, facecolor='r', alpha=0.3)\n","\n","axs[2].plot(ddpg_max_rt, 'r', linewidth=1)\n","axs[2].plot(ddpg_min_rt, 'r', linewidth=1)\n","axs[2].plot(ddpg_mean_rt, 'r:', label='DDPG', linewidth=2)\n","axs[2].fill_between(\n","    ddpg_x, ddpg_min_rt, ddpg_max_rt, facecolor='r', alpha=0.3)\n","\n","# ALL\n","axs[0].set_title('Total Steps')\n","axs[1].set_title('Training Time')\n","axs[2].set_title('Wall-clock Time')\n","plt.xlabel('Episodes')\n","axs[0].legend(loc='upper left')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3KnwEjHfMj0m"},"source":["# TD3"]},{"cell_type":"code","metadata":{"id":"5wBkxxiNMj0n"},"source":["# Noise decay strategy for exploation\n","# There is a task here!\n","class NormalNoiseDecayStrategy():\n","    def __init__(self, bounds, init_noise_ratio=0.5, min_noise_ratio=0.1, decay_steps=10000):\n","      \"\"\"\n","      Initialize class\n","\n","      bounds = max-min bounds for noise\n","      init_noise_ratio = initial noise ratio\n","      min_noise_ratio = minimum noise ratio\n","      decay_steps=10000 = noise decay steps\n","      \"\"\"\n","      self.t = 0\n","      self.low, self.high = bounds\n","      self.noise_ratio = init_noise_ratio\n","      self.init_noise_ratio = init_noise_ratio\n","      self.min_noise_ratio = min_noise_ratio\n","      self.decay_steps = decay_steps\n","      self.ratio_noise_injected = 0\n","\n","    def _noise_ratio_update(self):\n","      \"\"\"\n","      Update noise ratio \n","      \"\"\"\n","      noise_ratio = 1 - self.t / self.decay_steps\n","      noise_ratio = (self.init_noise_ratio - self.min_noise_ratio) * noise_ratio + self.min_noise_ratio\n","      noise_ratio = np.clip(noise_ratio, self.min_noise_ratio, self.init_noise_ratio)\n","      self.t += 1\n","      return noise_ratio\n","\n","    def select_action(self, model, state, max_exploration=False):\n","      \"\"\"\n","      Select noisy action for exploration\n","\n","      state = state in the environment\n","      max_exploration = noise scale selection\n","\n","      \"\"\"\n","      if max_exploration:\n","          noise_scale = self.high\n","      else:\n","          noise_scale = self.noise_ratio * self.high\n","\n","      with torch.no_grad():\n","          greedy_action = model(state).cpu().detach().data.numpy().squeeze()\n","\n","      noise = np.random.normal(loc=0, scale=noise_scale, size=len(self.high))\n","\n","      #TODO: compute the TD3 action with an exploration strategy. Use the values greedy_action and noise\n","      noisy_action = #To complete\n","\n","      #TODO use the np.clip function to clip the TD3 action between the values 'self.low' and 'self.high'\n","      action = # To complete. You may find more information about the clip function in https://numpy.org/doc/stable/reference/generated/numpy.clip.html\n","\n","      self.noise_ratio = self._noise_ratio_update()\n","      self.ratio_noise_injected = np.mean(abs((greedy_action - action)/(self.high - self.low)))\n","      return action"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1OmHF-3aMj0o"},"source":["# Plot noise decay function\n","# --- DO NOT TOUCH ---\n","s = NormalNoiseDecayStrategy(([-2],[2]))\n","plt.plot([s._noise_ratio_update() for _ in range(50000)])\n","plt.title('Normal Noise Linear ratio')\n","plt.xticks(rotation=45)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"M-GJb5VlMj0p"},"source":["# Q-value neural network class - Double Q-learning\n","# There is a task here!\n","\n","class FCTQV(nn.Module):\n","    def __init__(self, \n","                 input_dim, \n","                 output_dim):\n","        \"\"\"\n","        Class initialization\n","\n","        input_dim = input dimension\n","        output_dim = output dimension\n","\n","        hidden_dims = dimension for hidden layers\n","        activation_fc = activation function\n","\n","        device = processing device\n","        \"\"\"\n","        super(FCTQV, self).__init__()\n","        \n","        #TODO: Choose a non-linear activation function from https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity\n","        activation_fc = #To complete. Use the format ---> F.activation_function\n","        self.activation_fc = activation_fc\n","\n","        #TODO: propose the dimensions for the hidden layers\n","        hidden_dims = #To complete. Use the same format (dimension_1, ..., dimension_n)\n","\n","        # Initialize Q-value neural network A\n","        self.input_layer_a = nn.Linear(input_dim + output_dim, hidden_dims[0])\n","        self.hidden_layers_a = nn.ModuleList()\n","\n","        for i in range(len(hidden_dims)-1):\n","            hidden_layer_a = nn.Linear(hidden_dims[i], hidden_dims[i+1])\n","            self.hidden_layers_a.append(hidden_layer_a)\n","\n","        self.output_layer_a = nn.Linear(hidden_dims[-1], 1)\n","        \n","        #TODO: initialize the second Q-value neural network - Use the format nn_property_b\n","        #---YOUR CODE GOES HERE---\n","\n","\n","\n","\n","\n","\n","\n","\n","        #-------------------------\n","\n","        device = \"cpu\"\n","        if torch.cuda.is_available():\n","            device = \"cuda:0\"\n","        self.device = torch.device(device)\n","        self.to(self.device)\n","\n","    def _format(self, state, action):\n","        \"\"\"\n","        Format the state for pytorch\n","\n","        state = state from environment\n","        action = action from policy\n","        \"\"\"\n","        x, u = state, action\n","        if not isinstance(x, torch.Tensor):\n","            x = torch.tensor(x, \n","                             device=self.device, \n","                             dtype=torch.float32)\n","            x = x.unsqueeze(0)\n","        if not isinstance(u, torch.Tensor):\n","            u = torch.tensor(u, \n","                             device=self.device, \n","                             dtype=torch.float32)\n","            u = u.unsqueeze(0)\n","        return x, u\n","\n","    def forward(self, state, action):\n","        \"\"\"\n","        Forward function for neural network - Q-value\n","\n","        state = state from environment\n","        action = action from policy\n","        \"\"\"\n","        x, u = self._format(state, action)\n","        x = torch.cat((x, u), dim=1)\n","\n","        xa = self.activation_fc(self.input_layer_a(x))\n","        xb = self.activation_fc(self.input_layer_b(x))\n","\n","        for hidden_layer_a, hidden_layer_b in zip(self.hidden_layers_a, self.hidden_layers_b):\n","            xa = self.activation_fc(hidden_layer_a(xa))\n","            xb = self.activation_fc(hidden_layer_b(xb))\n","        \n","        xa = self.output_layer_a(xa)\n","        xb = self.output_layer_b(xb)\n","        return xa, xb\n","    \n","    def Qa(self, state, action):\n","        \"\"\"\n","        Forward function for neural network - policy\n","\n","        state = state from environment\n","        action = action from policy\n","        \"\"\"\n","        x, u = self._format(state, action)\n","        x = torch.cat((x, u), dim=1)\n","        xa = self.activation_fc(self.input_layer_a(x))\n","        for hidden_layer_a in self.hidden_layers_a:\n","            xa = self.activation_fc(hidden_layer_a(xa))\n","        return self.output_layer_a(xa)\n","    \n","    def load(self, experiences):\n","      \"\"\"\n","      load samples from experience - replay buffer database\n","\n","      experiences = samples from the replay buffer database\n","      \"\"\"\n","      states, actions, new_states, rewards, is_terminals = experiences\n","      states = torch.from_numpy(states).float().to(self.device)\n","      actions = torch.from_numpy(actions).float().to(self.device)\n","      new_states = torch.from_numpy(new_states).float().to(self.device)\n","      rewards = torch.from_numpy(rewards).float().to(self.device)\n","      is_terminals = torch.from_numpy(is_terminals).float().to(self.device)\n","      return states, actions, new_states, rewards, is_terminals"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ImNiBNLYMj0q"},"source":["# TD3 agent class\n","# There is a task here!\n","\n","class TD3():\n","    def __init__(self, \n","                 replay_buffer_fn,\n","                 policy_model_fn, \n","                 policy_max_grad_norm, \n","                 policy_optimizer_fn, \n","                 policy_optimizer_lr,\n","                 value_model_fn, \n","                 value_max_grad_norm, \n","                 value_optimizer_fn, \n","                 value_optimizer_lr, \n","                 training_strategy_fn,\n","                 evaluation_strategy_fn,\n","                 n_warmup_batches,\n","                 update_value_target_every_steps,\n","                 update_policy_target_every_steps,\n","                 train_policy_every_steps,\n","                 tau,\n","                 policy_noise_ratio,\n","                 policy_noise_clip_ratio):\n","      \"\"\"\n","      Class initialization\n","\n","      replay_buffer_fn = replay buffer function\n","\n","      policy_model_fn = policy neural network architecture\n","      policy_max_grad_norm = maximum gradient norm for policy model\n","      policy_optimizer_fn = optimizer for policy neural network\n","      policy_optimizer_lr = learning rate for policy neural network\n","\n","      value_model_fn = value function neural network architecture\n","      value_max_grad_norm = maximum gradient norm for Q-value model\n","      value_optimizer_fn = optimizer for value function neural network\n","      value_optimizer_lr = learning rate for value function neural network\n","\n","      training_strategy_fn = exploration strategy - Normal Noise Strategy\n","      evaluation_strategy_fn = evaluation strategy - Greedy Strategy\n","      n_warmup_batches = warm up batches for training\n","      update_value_target_every_steps = update frecuency for Q-value model\n","      update_policy_target_every_steps = update frecuency for policy model\n","      tau = Polyak averaging factor\n","      policy_noise_ratio = noise ratio for policy exploration\n","      policy_noise_clip_ratio = upper and lower bounds for actions\n","      \"\"\"\n","      self.replay_buffer_fn = replay_buffer_fn\n","\n","      self.policy_model_fn = policy_model_fn\n","      self.policy_max_grad_norm = policy_max_grad_norm\n","      self.policy_optimizer_fn = policy_optimizer_fn\n","      self.policy_optimizer_lr = policy_optimizer_lr\n","      \n","      self.value_model_fn = value_model_fn\n","      self.value_max_grad_norm = value_max_grad_norm\n","      self.value_optimizer_fn = value_optimizer_fn\n","      self.value_optimizer_lr = value_optimizer_lr\n","\n","      self.training_strategy_fn = training_strategy_fn\n","      self.evaluation_strategy_fn = evaluation_strategy_fn\n","\n","      self.n_warmup_batches = n_warmup_batches\n","      self.update_value_target_every_steps = update_value_target_every_steps\n","      self.update_policy_target_every_steps = update_policy_target_every_steps\n","      self.train_policy_every_steps = train_policy_every_steps\n","      \n","      self.tau = tau\n","      self.policy_noise_ratio = policy_noise_ratio\n","      self.policy_noise_clip_ratio = policy_noise_clip_ratio\n","\n","    def optimize_model(self, experiences):\n","      \"\"\"\n","      Optimize and update parameters in neural network models (Q value and policy models)\n","\n","      experiences= experience buffer replay - Database\n","      \n","      argmax_a_q_sp_a = greedy policy for next state in model a\n","      argmax_a_q_sp_b = greedy policy for next state in model b\n","      max_a_q_sp = max Q value function for next state\n","      target_q_sa = target Q value function\n","      q_sa = current Q value function\n","      td_error_a = TD error for model a\n","      td_error_b = TD error for model b\n","      value_loss = loss function for Q value function neural network model\n","\n","      argmax_a_q_s = greedy action for current state\n","      noisy_argmax_a_q_sp\n","      max_a_q_s = Q value from greedy action in current state\n","      policy_loss = loss function for policy neural network model\n","\n","      a_ran = range of actions\n","      a_noise = noise for exploration with actions\n","      n_min = minimum value for actions\n","      n_max = maximum value for actions\n","      a_noise = clipped noise for exploration\n","      \"\"\"\n","      # Get samples from replay buffer \n","      states, actions, rewards, next_states, is_terminals = experiences\n","      batch_size = len(is_terminals)\n","\n","      with torch.no_grad():\n","        \n","        # Compute noise for exploration with action\n","        a_ran = self.target_policy_model.env_max - self.target_policy_model.env_min\n","        a_noise = torch.randn_like(actions) * self.policy_noise_ratio * a_ran\n","        n_min = self.target_policy_model.env_min * self.policy_noise_clip_ratio\n","        n_max = self.target_policy_model.env_max * self.policy_noise_clip_ratio  \n","\n","        #TODO: apply the clipping function to a_noise by using n_min and n_max. You may want to use torch.max and torch.min\n","        a_noise = #To complete\n","\n","        # Compute action from policy model\n","        argmax_a_q_sp = self.target_policy_model(next_states)\n","\n","        #TODO: use argmax_a_q_sp and a_noise to compute the noisy action\n","        noisy_argmax_a_q_sp = # To complete\n","\n","        noisy_argmax_a_q_sp = torch.max(torch.min(noisy_argmax_a_q_sp, \n","                                                  self.target_policy_model.env_max),\n","                                        self.target_policy_model.env_min)\n","\n","        # Compute Q-value functions with models a and b \n","        max_a_q_sp_a, max_a_q_sp_b = self.target_value_model(next_states, noisy_argmax_a_q_sp)\n","\n","        #TODO: compute the target Q value function using max_a_q_sp_a and max_a_q_sp_b. You may want to use torch.min\n","        max_a_q_sp = # To complete --> Check out the agorithm in slide 32 from lecture 13\n","\n","        target_q_sa = rewards + self.gamma * max_a_q_sp * (1 - is_terminals)\n","\n","      # Compute current Q value function for models a and b\n","      q_sa_a, q_sa_b = self.online_value_model(states, actions)\n","\n","      #TODO: compute TD error for neural network a with target_q_sa and q_sa_a, and q_sa_b\n","      td_error_a = # To complete\n","      td_error_b = # To complete\n","\n","      #TODO: compute the value loss function to train the neural network, use td_error_a and td_error_b (check out slide 35 from lecture 13)\n","      value_loss = # To complete ---> Recall that we are dealing with td error samples, don't forget to compute the expectation via Monte Carlo\n","      \n","      # Backpropagation for value function neural network\n","      self.value_optimizer.zero_grad()\n","      value_loss.backward()\n","      torch.nn.utils.clip_grad_norm_(self.online_value_model.parameters(), \n","                                      self.value_max_grad_norm)\n","      self.value_optimizer.step()\n","\n","      if np.sum(self.episode_timestep) % self.train_policy_every_steps == 0:\n","        # Compute greedy action with policy model for current state\n","        argmax_a_q_s = self.online_policy_model(states)\n","\n","        # Get Q value from greedy action and current state\n","        max_a_q_s = self.online_value_model.Qa(states, argmax_a_q_s)\n","\n","        # Compute loss for policy model\n","        policy_loss = -max_a_q_s.mean()\n","\n","        # Backpropagation for policy neural network\n","        self.policy_optimizer.zero_grad()\n","        policy_loss.backward()\n","        torch.nn.utils.clip_grad_norm_(self.online_policy_model.parameters(), \n","                                        self.policy_max_grad_norm)        \n","        self.policy_optimizer.step()\n","\n","    def interaction_step(self, state, env):\n","      \"\"\"\n","      agent interacts with the environment by applying action\n","\n","      min_samples = database from the buffer replay - Train neural networks\n","      action = action from the policy model\n","      new_state = new state in the environment\n","      reward = reward from the current interaction\n","      is_failure = flag for terminal state\n","      experience = new sample tuple (state, action, reward, new_state, is_failure) for buffer replay\n","      \"\"\"\n","      min_samples = self.replay_buffer.batch_size * self.n_warmup_batches\n","      action = self.training_strategy.select_action(self.online_policy_model, \n","                                                    state, \n","                                                    len(self.replay_buffer) < min_samples)\n","      new_state, reward, is_terminal, info = env.step(action)\n","      is_truncated = 'TimeLimit.truncated' in info and info['TimeLimit.truncated']\n","      is_failure = is_terminal and not is_truncated\n","      experience = (state, action, reward, new_state, float(is_failure))\n","\n","      self.replay_buffer.store(experience)\n","      self.episode_reward[-1] += reward\n","      self.episode_timestep[-1] += 1\n","      self.episode_exploration[-1] += self.training_strategy.ratio_noise_injected\n","      return new_state, is_terminal\n","\n","    def update_value_network(self, tau=None):\n","      \"\"\"\n","      Update Q-value neural network models parameters (policy and Q-value function) via Polyak Averaging\n","      * We use this technique to avoid aggressive model parameters updates\n","\n","      tau = Polyak averaging factor\n","      target_ratio = averaging ratio\n","      mixed_weights = new mixed weights\n","      \"\"\"\n","      tau = self.tau if tau is None else tau\n","      for target, online in zip(self.target_value_model.parameters(), \n","                                self.online_value_model.parameters()):\n","          target_ratio = (1.0 - self.tau) * target.data\n","          online_ratio = self.tau * online.data\n","          mixed_weights = target_ratio + online_ratio\n","          target.data.copy_(mixed_weights)\n","\n","    def update_policy_network(self, tau=None):\n","      \"\"\"\n","      Update Q-value neural network models parameters (policy and Q-value function) via Polyak Averaging\n","      * We use this technique to avoid aggressive model parameters updates\n","\n","      tau = Polyak averaging factor\n","      target_ratio = averaging ratio\n","      mixed_weights = new mixed weights\n","      \"\"\"\n","      tau = self.tau if tau is None else tau\n","      for target, online in zip(self.target_policy_model.parameters(), \n","                                self.online_policy_model.parameters()):\n","          target_ratio = (1.0 - self.tau) * target.data\n","          online_ratio = self.tau * online.data\n","          mixed_weights = target_ratio + online_ratio\n","          target.data.copy_(mixed_weights)\n","\n","    def train(self, make_env_fn, make_env_kargs, seed, gamma, \n","              max_minutes, max_episodes, goal_mean_100_reward):\n","      \"\"\"\n","      Training function and computing stats \n","\n","      make_env_fn = make environment function\n","      make_env_kargs = arguments for make environment function\n","      seed = seed for random numbers\n","      gamma = discount factor\n","      max_minutes = maximum training time\n","      max_episodes = maximum training episodes\n","      goal_mean_100_reward = target reward goal\n","      \"\"\"\n","      # Setup environment\n","      training_start, last_debug_time = time.time(), float('-inf')\n","\n","      self.checkpoint_dir = tempfile.mkdtemp()\n","      self.make_env_fn = make_env_fn\n","      self.make_env_kargs = make_env_kargs\n","      self.seed = seed\n","      self.gamma = gamma\n","      \n","      env = self.make_env_fn(**self.make_env_kargs, seed=self.seed)\n","      torch.manual_seed(self.seed) ; np.random.seed(self.seed) ; random.seed(self.seed)\n","\n","      # Initialize actor-critic agent\n","      nS, nA = env.observation_space.shape[0], env.action_space.shape[0]\n","      action_bounds = env.action_space.low, env.action_space.high\n","      self.episode_timestep = []\n","      self.episode_reward = []\n","      self.episode_seconds = []\n","      self.evaluation_scores = []        \n","      self.episode_exploration = []\n","      \n","      # Setup target and online Q-value functions\n","      self.target_value_model = self.value_model_fn(nS, nA)\n","      self.online_value_model = self.value_model_fn(nS, nA)\n","      self.update_value_network(tau=1.0)\n","\n","      # Setup target and online policy model\n","      self.target_policy_model = self.policy_model_fn(nS, action_bounds)\n","      self.online_policy_model = self.policy_model_fn(nS, action_bounds)\n","      self.update_policy_network(tau=1.0)\n","\n","      # Setup optimize and update parameters functions\n","      self.value_optimizer = self.value_optimizer_fn(self.online_value_model, \n","                                                      self.value_optimizer_lr)        \n","      self.policy_optimizer = self.policy_optimizer_fn(self.online_policy_model, \n","                                                        self.policy_optimizer_lr)\n","\n","      # Setup replay buffer, and training/evaluation strategies\n","      self.replay_buffer = self.replay_buffer_fn()\n","      self.training_strategy = training_strategy_fn(action_bounds)\n","      self.evaluation_strategy = evaluation_strategy_fn(action_bounds)\n","                  \n","      result = np.empty((max_episodes, 5))\n","      result[:] = np.nan\n","      training_time = 0\n","\n","      # Episodic interaction agent-environment \n","      for episode in range(1, max_episodes + 1):\n","          episode_start = time.time()\n","          \n","          state, is_terminal = env.reset(), False\n","          self.episode_reward.append(0.0)\n","          self.episode_timestep.append(0.0)\n","          self.episode_exploration.append(0.0)\n","\n","          for step in count():\n","              state, is_terminal = self.interaction_step(state, env)\n","\n","              min_samples = self.replay_buffer.batch_size * self.n_warmup_batches\n","              if len(self.replay_buffer) > min_samples:\n","                  experiences = self.replay_buffer.sample()\n","                  experiences = self.online_value_model.load(experiences)\n","                  self.optimize_model(experiences)\n","\n","              if np.sum(self.episode_timestep) % self.update_value_target_every_steps == 0:\n","                  self.update_value_network()\n","\n","              if np.sum(self.episode_timestep) % self.update_policy_target_every_steps == 0:\n","                  self.update_policy_network()\n","\n","              if is_terminal:\n","                  gc.collect()\n","                  break\n","          \n","          # save stats\n","          #---DO NOT TOUCH---\n","          episode_elapsed = time.time() - episode_start\n","          self.episode_seconds.append(episode_elapsed)\n","          training_time += episode_elapsed\n","          evaluation_score, _ = self.evaluate(self.online_policy_model, env)\n","          self.save_checkpoint(episode-1, self.online_policy_model)\n","\n","          total_step = int(np.sum(self.episode_timestep))\n","          self.evaluation_scores.append(evaluation_score)\n","          \n","          mean_10_reward = np.mean(self.episode_reward[-10:])\n","          std_10_reward = np.std(self.episode_reward[-10:])\n","          mean_100_reward = np.mean(self.episode_reward[-100:])\n","          std_100_reward = np.std(self.episode_reward[-100:])\n","          mean_100_eval_score = np.mean(self.evaluation_scores[-100:])\n","          std_100_eval_score = np.std(self.evaluation_scores[-100:])\n","          lst_100_exp_rat = np.array(\n","              self.episode_exploration[-100:])/np.array(self.episode_timestep[-100:])\n","          mean_100_exp_rat = np.mean(lst_100_exp_rat)\n","          std_100_exp_rat = np.std(lst_100_exp_rat)\n","          \n","          wallclock_elapsed = time.time() - training_start\n","          result[episode-1] = total_step, mean_100_reward, \\\n","              mean_100_eval_score, training_time, wallclock_elapsed\n","          \n","          reached_debug_time = time.time() - last_debug_time >= LEAVE_PRINT_EVERY_N_SECS\n","          reached_max_minutes = wallclock_elapsed >= max_minutes * 60\n","          reached_max_episodes = episode >= max_episodes\n","          reached_goal_mean_reward = mean_100_eval_score >= goal_mean_100_reward\n","          training_is_over = reached_max_minutes or \\\n","                              reached_max_episodes or \\\n","                              reached_goal_mean_reward\n","          elapsed_str = time.strftime(\"%H:%M:%S\", time.gmtime(time.time() - training_start))\n","          debug_message = 'el {}, ep {:04}, ts {:07}, '\n","          debug_message += 'ar 10 {:05.1f}\\u00B1{:05.1f}, '\n","          debug_message += '100 {:05.1f}\\u00B1{:05.1f}, '\n","          debug_message += 'ex 100 {:02.1f}\\u00B1{:02.1f}, '\n","          debug_message += 'ev {:05.1f}\\u00B1{:05.1f}'\n","          debug_message = debug_message.format(\n","              elapsed_str, episode-1, total_step, mean_10_reward, std_10_reward, \n","              mean_100_reward, std_100_reward, mean_100_exp_rat, std_100_exp_rat,\n","              mean_100_eval_score, std_100_eval_score)\n","          print(debug_message, end='\\r', flush=True)\n","          if reached_debug_time or training_is_over:\n","              print(ERASE_LINE + debug_message, flush=True)\n","              last_debug_time = time.time()\n","          if training_is_over:\n","              if reached_max_minutes: print(u'--> reached_max_minutes \\u2715')\n","              if reached_max_episodes: print(u'--> reached_max_episodes \\u2715')\n","              if reached_goal_mean_reward: print(u'--> reached_goal_mean_reward \\u2713')\n","              break\n","      \n","      # End training and save results      \n","      final_eval_score, score_std = self.evaluate(self.online_policy_model, env, n_episodes=100)\n","      wallclock_time = time.time() - training_start\n","      print('Training complete.')\n","      print('Final evaluation score {:.2f}\\u00B1{:.2f} in {:.2f}s training time,'\n","            ' {:.2f}s wall-clock time.\\n'.format(\n","                final_eval_score, score_std, training_time, wallclock_time))\n","      env.close() ; del env\n","      self.get_cleaned_checkpoints()\n","      return result, final_eval_score, training_time, wallclock_time\n","    \n","    def evaluate(self, eval_policy_model, eval_env, n_episodes=1):\n","      \"\"\"\n","      evaluate trained policy\n","      \n","      eval_policy_model = policy model to evaluate\n","      eval_env = environment to evaluate\n","      n_episodes = number of episodes to evaluate\n","      a = action\n","      s = current state\n","      r = reward\n","      d = next state\n","      \"\"\"\n","      rs = []\n","      for _ in range(n_episodes):\n","          s, d = eval_env.reset(), False\n","          rs.append(0)\n","          for _ in count():\n","              a = self.evaluation_strategy.select_action(eval_policy_model, s)\n","              s, r, d, _ = eval_env.step(a)\n","              rs[-1] += r\n","              if d: break\n","      return np.mean(rs), np.std(rs)\n","\n","    def get_cleaned_checkpoints(self, n_checkpoints=4):\n","      \"\"\"\n","      clean database for saving\n","      \"\"\"\n","      try: \n","          return self.checkpoint_paths\n","      except AttributeError:\n","          self.checkpoint_paths = {}\n","\n","      paths = glob.glob(os.path.join(self.checkpoint_dir, '*.tar'))\n","      paths_dic = {int(path.split('.')[-2]):path for path in paths}\n","      last_ep = max(paths_dic.keys())\n","      checkpoint_idxs = np.linspace(1, last_ep+1, n_checkpoints, endpoint=True, dtype=np.int)-1\n","\n","      for idx, path in paths_dic.items():\n","          if idx in checkpoint_idxs:\n","              self.checkpoint_paths[idx] = path\n","          else:\n","              os.unlink(path)\n","\n","      return self.checkpoint_paths\n","\n","    def save_checkpoint(self, episode_idx, model):\n","      \"\"\"\n","      Save model\n","      \"\"\"\n","      torch.save(model.state_dict(), \n","                  os.path.join(self.checkpoint_dir, 'model.{}.tar'.format(episode_idx)))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qVO4awI1Mj0r","scrolled":false},"source":["# TD3 training/evaluation routine\n","# There is a task here\n","td3_results = []\n","best_agent, best_eval_score = None, float('-inf')\n","\n","# Prepare environment\n","for seed in SEEDS:\n","    environment_settings = {\n","        'env_name': 'Pendulum-v0',\n","        'gamma': 0.99,\n","        'max_minutes': 5,\n","        'max_episodes': 500,\n","        'goal_mean_100_reward': -150\n","    }\n","\n","    # Setup TD3 agent\n","    #---Policy neural network---\n","    policy_model_fn = lambda nS, bounds: FCDP(nS, bounds)\n","    policy_max_grad_norm = float('inf')\n","    \n","    #TODO: select an optimization algorithm for the policy neural network. For further information: https://pytorch.org/docs/stable/optim.html - Algorithms\n","    policy_optimizer_fn = lambda net, lr: # To complete -> Follow the format optim.Optimization_Algorithm(net.parameters(), lr=lr)\n","\n","    #TODO: select a suitable learning rate for the optimization algorithm\n","    policy_optimizer_lr = # To complete\n","\n","    #---Value function neural network---\n","    value_model_fn = lambda nS, nA: FCTQV(nS, nA)\n","    value_max_grad_norm = float('inf')\n","\n","    #TODO: select an optimization algorithm for the value neural network. For further information: https://pytorch.org/docs/stable/optim.html - Algorithms\n","    value_optimizer_fn = lambda net, lr: # To complete -> Follow the format optim.Optimization_Algorithm(net.parameters(), lr=lr)\n","\n","    #TODO: select a suitable learning rate for the optimization algorithm\n","    value_optimizer_lr = # To complete\n","\n","    # Training/evaluation strategy\n","    training_strategy_fn = lambda bounds: NormalNoiseDecayStrategy(bounds,\n","                                                                   init_noise_ratio=0.5,\n","                                                                   min_noise_ratio=0.1,\n","                                                                   decay_steps=200000)\n","    evaluation_strategy_fn = lambda bounds: GreedyStrategy(bounds)\n","\n","    replay_buffer_fn = lambda: ReplayBuffer(max_size=1000000, batch_size=256)\n","    n_warmup_batches = 5\n","    update_value_target_every_steps = 2\n","    update_policy_target_every_steps = 2\n","    train_policy_every_steps = 2\n","    policy_noise_ratio = 0.1\n","    policy_noise_clip_ratio = 0.5\n","    tau = 0.005\n","\n","    env_name, gamma, max_minutes, \\\n","    max_episodes, goal_mean_100_reward = environment_settings.values()\n","\n","    # Update agent\n","    agent = TD3(replay_buffer_fn,\n","                policy_model_fn, \n","                policy_max_grad_norm, \n","                policy_optimizer_fn, \n","                policy_optimizer_lr,\n","                value_model_fn,\n","                value_max_grad_norm, \n","                value_optimizer_fn, \n","                value_optimizer_lr, \n","                training_strategy_fn,\n","                evaluation_strategy_fn,\n","                n_warmup_batches,\n","                update_value_target_every_steps,\n","                update_policy_target_every_steps,\n","                train_policy_every_steps,\n","                tau,\n","                policy_noise_ratio,\n","                policy_noise_clip_ratio)\n","    # Train/evaluate agent\n","    make_env_fn, make_env_kargs = get_make_env_fn(env_name=env_name)\n","\n","    result, final_eval_score, training_time, wallclock_time = agent.train(\n","        make_env_fn, make_env_kargs, seed, gamma, max_minutes, max_episodes, goal_mean_100_reward)\n","    \n","    # Save results\n","    td3_results.append(result)\n","\n","    if final_eval_score > best_eval_score:\n","        best_eval_score = final_eval_score\n","        best_agent = agent\n","\n","td3_results = np.array(td3_results)\n","_ = BEEP()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cFzaPt40Mj0s"},"source":["# Save results from TD3 agent \n","# ---DO NOT TOUCH---\n","td3_max_t, td3_max_r, td3_max_s, \\\n","td3_max_sec, td3_max_rt = np.max(td3_results, axis=0).T\n","td3_min_t, td3_min_r, td3_min_s, \\\n","td3_min_sec, td3_min_rt = np.min(td3_results, axis=0).T\n","td3_mean_t, td3_mean_r, td3_mean_s, \\\n","td3_mean_sec, td3_mean_rt = np.mean(td3_results, axis=0).T\n","td3_x = np.arange(len(td3_mean_s))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9xViE0sfMj0s"},"source":["# Plot results\n","# ---DO NOT TOUCH---\n","fig, axs = plt.subplots(2, 1, figsize=(15,10), sharey=False, sharex=True)\n","\n","# TD3\n","axs[0].plot(td3_max_r, 'b', linewidth=1)\n","axs[0].plot(td3_min_r, 'b', linewidth=1)\n","axs[0].plot(td3_mean_r, 'b:', label='TD3', linewidth=2)\n","axs[0].fill_between(\n","    td3_x, td3_min_r, td3_max_r, facecolor='b', alpha=0.3)\n","\n","axs[1].plot(td3_max_s, 'b', linewidth=1)\n","axs[1].plot(td3_min_s, 'b', linewidth=1)\n","axs[1].plot(td3_mean_s, 'b:', label='TD3', linewidth=2)\n","axs[1].fill_between(\n","    td3_x, td3_min_s, td3_max_s, facecolor='b', alpha=0.3)\n","\n","# ALL\n","axs[0].set_title('Moving Avg Reward (Training)')\n","axs[1].set_title('Moving Avg Reward (Evaluation)')\n","plt.xlabel('Episodes')\n","axs[0].legend(loc='upper left')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9LXruSW4Mj0t"},"source":["# Plot results\n","# ---DO NOT TOUCH---\n","fig, axs = plt.subplots(3, 1, figsize=(15,15), sharey=False, sharex=True)\n","\n","# TD3\n","axs[0].plot(td3_max_t, 'b', linewidth=1)\n","axs[0].plot(td3_min_t, 'b', linewidth=1)\n","axs[0].plot(td3_mean_t, 'b:', label='TD3', linewidth=2)\n","axs[0].fill_between(\n","    td3_x, td3_min_t, td3_max_t, facecolor='b', alpha=0.3)\n","\n","axs[1].plot(td3_max_sec, 'b', linewidth=1)\n","axs[1].plot(td3_min_sec, 'b', linewidth=1)\n","axs[1].plot(td3_mean_sec, 'b:', label='TD3', linewidth=2)\n","axs[1].fill_between(\n","    td3_x, td3_min_sec, td3_max_sec, facecolor='b', alpha=0.3)\n","\n","axs[2].plot(td3_max_rt, 'b', linewidth=1)\n","axs[2].plot(td3_min_rt, 'b', linewidth=1)\n","axs[2].plot(td3_mean_rt, 'b:', label='TD3', linewidth=2)\n","axs[2].fill_between(\n","    td3_x, td3_min_rt, td3_max_rt, facecolor='b', alpha=0.3)\n","\n","# ALL\n","axs[0].set_title('Total Steps')\n","axs[1].set_title('Training Time')\n","axs[2].set_title('Wall-clock Time')\n","plt.xlabel('Episodes')\n","axs[0].legend(loc='upper left')\n","plt.show()"],"execution_count":null,"outputs":[]}]}